{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f164dac9-2ac7-443d-bdba-7d04aa32987a",
   "metadata": {},
   "source": [
    "# **1 Define vars**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7a0fd6-d590-4466-a825-9c7b77ef2cc1",
   "metadata": {},
   "source": [
    "## **1.1 Define var name and path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62964675-25ff-4061-a306-dbd46e4786a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable = {\n",
    "    \"xco2\": \"/data3/interns/NRT_CO2_Emission_Map_Project/HaoHu_work/XCO2_resample/global_grid_0.1_2019_2025_xco2.nc\",\n",
    "}\n",
    "feature_variables = {\n",
    "    \"t2m\": \"/data3/interns/NRT_CO2_Emission_Map_Project/HaoHu_work/ERA5_resample/t2m_daily_0p1deg.nc\",\n",
    "    \"d2m\": \"/data3/interns/NRT_CO2_Emission_Map_Project/HaoHu_work/ERA5_resample/d2m_daily_0p1deg.nc\",\n",
    "    \"u10\": \"/data3/interns/NRT_CO2_Emission_Map_Project/HaoHu_work/ERA5_resample/u10_daily_0p1deg.nc\",\n",
    "    \"v10\": \"/data3/interns/NRT_CO2_Emission_Map_Project/HaoHu_work/ERA5_resample/v10_daily_0p1deg.nc\",\n",
    "    \"msl\": \"/data3/interns/NRT_CO2_Emission_Map_Project/HaoHu_work/ERA5_resample/msl_daily_0p1deg.nc\",\n",
    "    \"sp\": \"/data3/interns/NRT_CO2_Emission_Map_Project/HaoHu_work/ERA5_resample/sp_daily_0p1deg.nc\",\n",
    "    \"skt\": \"/data3/interns/NRT_CO2_Emission_Map_Project/HaoHu_work/ERA5_resample/skt_daily_0p1deg.nc\",\n",
    "    \"tp\": \"/data3/interns/NRT_CO2_Emission_Map_Project/HaoHu_work/ERA5_resample/tp_daily_0p1deg.nc\",\n",
    "    \"e\": \"/data3/interns/NRT_CO2_Emission_Map_Project/HaoHu_work/ERA5_resample/e_daily_0p1deg.nc\",\n",
    "    \"ssr\": \"/data3/interns/NRT_CO2_Emission_Map_Project/HaoHu_work/ERA5_resample/ssr_daily_0p1deg.nc\",\n",
    "    \"str\": \"/data3/interns/NRT_CO2_Emission_Map_Project/HaoHu_work/ERA5_resample/str_daily_0p1deg.nc\",\n",
    "    \"tcw\": \"/data3/interns/NRT_CO2_Emission_Map_Project/HaoHu_work/ERA5_resample/tcw_daily_0p1deg.nc\",\n",
    "    \"blh\": \"/data3/interns/NRT_CO2_Emission_Map_Project/HaoHu_work/ERA5_resample/blh_daily_0p1deg.nc\",\n",
    "    \n",
    "    \"NO2\": \"/data3/interns/NRT_CO2_Emission_Map_Project/HaoHu_work/XCO2_resample/global_grid_0.1_2019_2025_NO2.nc\",\n",
    "    \"is_weekend\": \"/data3/interns/NRT_CO2_Emission_Map_Project/HaoHu_work/XCO2_resample/global_grid_0.1_2019_2025_weekday_weekend.nc\",\n",
    "    \"population\": \"/data3/interns/NRT_CO2_Emission_Map_Project/MingjuanZhang_work/Population_global_0.1degree_2019_2025_ns.nc\",\n",
    "    \"elevation\": \"/data3/interns/NRT_CO2_Emission_Map_Project/MingjuanZhang_work/SRTM_elevation_global_0.1degree_2019_2025_ns.nc\",\n",
    "    \"landuse\": \"/data3/interns/NRT_CO2_Emission_Map_Project/MingjuanZhang_work/Landuse_global_0.1degree_2019_2025_ns.nc\",\n",
    "    \"aspect\": \"/data3/interns/NRT_CO2_Emission_Map_Project/MingjuanZhang_work/SRTM_aspect_global_0.1degree_2019_2025_ns.nc\",\n",
    "    \"ndvi\": \"/data3/interns/NRT_CO2_Emission_Map_Project/MingjuanZhang_work/NDVI_global_0.1degree_2019_2025_ns.nc\",\n",
    "    \"gpp\": \"/data3/interns/NRT_CO2_Emission_Map_Project/MingjuanZhang_work/GPP_global_0.1degree_2019_2025_ns.nc\",\n",
    "    \"lai\": \"/data3/interns/NRT_CO2_Emission_Map_Project/MingjuanZhang_work/LAI_global_0.1degree_2019_2025_ns.nc\",\n",
    "    \"ntl\": \"/data3/interns/NRT_CO2_Emission_Map_Project/MingjuanZhang_work/VIIRS_NTL_global_0.1degree_2019_2025_ns.nc\",\n",
    "    \"evi\": \"/data3/interns/NRT_CO2_Emission_Map_Project/MingjuanZhang_work/EVI_global_0.1degree_2019_2025_ns.nc\",\n",
    "    \"slope\": \"/data3/interns/NRT_CO2_Emission_Map_Project/MingjuanZhang_work/SRTM_slope_global_0.1degree_2019_2025_ns.nc\",\n",
    "    \"odiac\": \"/data3/interns/NRT_CO2_Emission_Map_Project/HaoHu_work/odiac_interp_2019_2025.nc\",\n",
    "    \"CO2_fire\": \"/data3/interns/NRT_CO2_Emission_Map_Project/PinyiLu_work/GFAS_resample/GFAS_resample_final.nc\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3850223b-1718-404d-8e7b-92f6aba07f2e",
   "metadata": {},
   "source": [
    "## **1.2 Load modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6041a9d0-d2bf-4f34-95f6-9fdefb21ccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import gc\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import xgboost as xgb\n",
    "import geopandas as gpd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ece4cc-066b-4639-bd32-ebdaca5610d1",
   "metadata": {},
   "source": [
    "# **2 Split the data to be monthly and Save**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2acb678",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vars = {\n",
    "    \"time\": None,\n",
    "    \"lat\": None,\n",
    "    \"lon\": None,\n",
    "    **target_variable,\n",
    "    **feature_variables\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa98cc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "npy_dir = \"/data3/interns/NRT_CO2_Emission_Map_Project/ML_XCO2/\"  \n",
    "out_dir = \"/data3/interns/NRT_CO2_Emission_Map_Project/ML_XCO2/monthly_slices/\" \n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "all_vars = {\"time\": None, \"lat\": None, \"lon\": None, **target_variable, **feature_variables}\n",
    "time_path = os.path.join(npy_dir, \"time.npy\")\n",
    "time_arr = np.load(time_path, mmap_mode=\"r\")\n",
    "time_dt = time_arr\n",
    "time_month = time_dt.astype(\"datetime64[M]\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c090c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Per-month processing:  90%|████████▉ | 69/77 [21:57:32<2:19:24, 1045.62s/it] "
     ]
    }
   ],
   "source": [
    "months = np.arange(np.datetime64('2019-01'),\n",
    "                   np.datetime64('2025-06'),  \n",
    "                   np.timedelta64(1, 'M'))\n",
    "\n",
    "outer_pbar = tqdm(months, desc=\"Per-month processing\")\n",
    "\n",
    "\n",
    "for ym in outer_pbar:\n",
    "    mask = (time_month == ym)\n",
    "    n_mask = int(mask.sum())\n",
    "    if n_mask == 0:\n",
    "        outer_pbar.set_postfix_str(f\"{ym}: no rows, skip\")\n",
    "        continue\n",
    "    cols = {}\n",
    "    time_month_slice = time_dt[mask]\n",
    "\n",
    "    cols[\"time\"] = time_month_slice\n",
    "    inner_pbar = tqdm(all_vars.keys(), leave=False, desc=f\"{ym} variables\")\n",
    "\n",
    "    for var_name in inner_pbar:\n",
    "        inner_pbar.set_postfix_str(var_name)\n",
    "\n",
    "        if var_name == \"time\":\n",
    "            continue\n",
    "\n",
    "        var_path = os.path.join(npy_dir, f\"{var_name}.npy\")\n",
    "\n",
    "        if not os.path.exists(var_path):\n",
    "            cols[var_name] = np.full(n_mask, np.nan, dtype=\"float64\")\n",
    "            continue\n",
    "        arr = np.load(var_path, mmap_mode=\"r\")\n",
    "        try:\n",
    "            sub = arr[mask]\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ {ym} {var_name}: Index failed({e})，Using NaN to fill\")\n",
    "            sub = np.full(n_mask, np.nan, dtype=\"float64\")\n",
    "\n",
    "        if np.issubdtype(sub.dtype, np.number):\n",
    "            cols[var_name] = sub\n",
    "        else:\n",
    "            try:\n",
    "                sub_num = pd.to_numeric(sub, errors=\"coerce\").astype(\"float64\")\n",
    "                cols[var_name] = sub_num\n",
    "            except Exception:\n",
    "                cols[var_name] = sub \n",
    "        del arr, sub\n",
    "        gc.collect()\n",
    "\n",
    "    for must in [\"lat\", \"lon\"]:\n",
    "        if must not in cols and must in all_vars:\n",
    "            vpath = os.path.join(npy_dir, f\"{must}.npy\")\n",
    "            if os.path.exists(vpath):\n",
    "                arr = np.load(vpath, mmap_mode=\"r\")\n",
    "                try:\n",
    "                    cols[must] = arr[mask]\n",
    "                except Exception:\n",
    "                    cols[must] = pd.to_numeric(arr, errors=\"coerce\")[mask]\n",
    "                del arr\n",
    "                gc.collect()\n",
    "    names = list(cols.keys())\n",
    "    arrays = [cols[k] for k in names]\n",
    "\n",
    "    lens = {k: len(v) for k, v in zip(names, arrays)}\n",
    "    if len(set(lens.values())) != 1:\n",
    "        print(f\"❌ Not consistent in length of row：{lens}；skip {ym}\")\n",
    "        del cols\n",
    "        gc.collect()\n",
    "        continue\n",
    "\n",
    "    rec = np.rec.fromarrays(arrays, names=names)\n",
    "    out_path = os.path.join(out_dir, f\"{str(ym).replace('-', '_')}.npy\")\n",
    "    np.save(out_path, rec)\n",
    "\n",
    "    del cols, arrays, rec, mask, time_month_slice\n",
    "    gc.collect()\n",
    "\n",
    "del time_arr, time_dt, time_month\n",
    "gc.collect()\n",
    "\n",
    "print(\"✅ All Completed：\", out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9c41b5",
   "metadata": {},
   "source": [
    "# **3. Prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b18ec4",
   "metadata": {},
   "source": [
    "## **Feature engineering like training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c99655f-fdbb-43e3-a93e-0f7ea63dd938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_value(df_month):\n",
    "\n",
    "    fill_map = {\n",
    "        \"population\": -2,\n",
    "        \"aspect\":    -2,\n",
    "        \"slope\":     -2,\n",
    "        \"ntl\":       -2,  \n",
    "        \"evi\":       -2,  \n",
    "        \"ndvi\":      -2,\n",
    "        \"gpp\":       -2,\n",
    "        \"lai\":       -2,\n",
    "    }\n",
    "\n",
    "    for col, val in fill_map.items():\n",
    "        if col in df_month.columns:\n",
    "            df_month[col].fillna(val, inplace=True)\n",
    "\n",
    "    return df_month\n",
    "\n",
    "def Process_LU(df_month):\n",
    "\n",
    "    df_month['landuse'] = df_month['landuse'].astype(str)\n",
    "\n",
    "    df_month = pd.get_dummies(df_month, columns=['landuse'], prefix='lu')\n",
    "\n",
    "    return df_month\n",
    "\n",
    "def time_transfer(df_month):\n",
    "    df_month[\"time\"] = pd.to_datetime(df_month[\"time\"])\n",
    "\n",
    "    df_month[\"year\"] = df_month[\"time\"].dt.year\n",
    "    df_month[\"month\"] = df_month[\"time\"].dt.month\n",
    "    df_month[\"day\"] = df_month[\"time\"].dt.day\n",
    "\n",
    "    return df_month\n",
    "\n",
    "def Process_geo_month(df_month):\n",
    "\n",
    "    df_month[\"month_sin\"] = np.sin(2 * np.pi * df_month[\"month\"] / 12)\n",
    "    df_month[\"month_cos\"] = np.cos(2 * np.pi * df_month[\"month\"] / 12)\n",
    "\n",
    "    df_month[\"geo_x\"] = np.cos(np.radians(df_month[\"lat\"])) * np.cos(np.radians(df_month[\"lon\"]))\n",
    "    df_month[\"geo_y\"] = np.cos(np.radians(df_month[\"lat\"])) * np.sin(np.radians(df_month[\"lon\"]))\n",
    "    df_month[\"geo_z\"] = np.sin(np.radians(df_month[\"lat\"]))\n",
    "    \n",
    "\n",
    "    date = pd.to_datetime(df_month[['year','month','day']])\n",
    "    df_month['n_day'] = (date - pd.Timestamp('2019-01-01')).dt.days + 1\n",
    "\n",
    "    return df_month\n",
    "\n",
    "def generate_features(df_month):\n",
    "    features = [col for col in df_month.columns if col not in [\"lat\", \"lon\", \"xco2\", \"time\", \"split\", \"time_bin\", \"lat_bin\", \"lon_bin\", \"spacetime_block\",\"month\", \"emission\"]]\n",
    "    return features\n",
    "\n",
    "def xgb_predict(df_month, features, model):\n",
    "\n",
    "    X = df_month[features]\n",
    "    df_month[\"xco2_pred\"] = model.predict(X)\n",
    "\n",
    "    keep_cols = [\"lat\", \"lon\", \"time\", \"xco2\", \"xco2_pred\"]\n",
    "\n",
    "    return df_month[keep_cols]\n",
    "\n",
    "def evaluate_prediction(df_month):\n",
    "    y_true = df_month[\"xco2\"].to_numpy()\n",
    "    y_pred = df_month[\"xco2_pred\"].to_numpy()\n",
    "    mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "    yt = y_true[mask]\n",
    "    yp = y_pred[mask]\n",
    "    err = yp - yt\n",
    "    bias = float(err.mean())                              \n",
    "    rmse = float(np.sqrt(mean_squared_error(yt, yp)))      \n",
    "    r2   = float(r2_score(yt, yp))                      \n",
    "    return bias, rmse, r2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e13a63",
   "metadata": {},
   "source": [
    "## **Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd82ab0-2d50-4cd7-b2cd-5097931d98c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_dir = Path(\"monthly_slices\")         \n",
    "out_pred_dir = Path(\"XCO2_prediction_full\")      \n",
    "out_pred_dir.mkdir(parents=True, exist_ok=True)\n",
    "metrics_path = out_pred_dir / \"metrics_2019_01_to_2025_06.csv\"\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "model.load_model(\"Trained_xgb_model_full/xgb_model_full_random.json\")\n",
    "\n",
    "months = pd.period_range(\"2019-01\", \"2025-06\", freq=\"M\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for p in tqdm(months, desc=\"Processing months\", unit=\"month\"):\n",
    "    tag = f\"{p.year}_{p.month:02d}\"\n",
    "    in_path = monthly_dir / f\"{tag}.npy\"\n",
    "\n",
    "    if not in_path.exists():\n",
    "        print(f\"[SKIP] {in_path} \")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        arr = np.load(in_path, allow_pickle=True)\n",
    "        df_month = pd.DataFrame(arr)\n",
    "\n",
    "        df_month = fill_missing_value(df_month)\n",
    "        df_month = Process_LU(df_month)\n",
    "        df_month = time_transfer(df_month)\n",
    "        df_month = Process_geo_month(df_month)\n",
    "        features = generate_features(df_month)\n",
    "\n",
    "        df_month = xgb_predict(df_month, features, model)\n",
    "\n",
    "        bias, rmse, r2 = evaluate_prediction(df_month)\n",
    "        nrows = len(df_month)\n",
    "        print(f\"{tag}: Bias={bias:.4f} | RMSE={rmse:.4f} | R^2={r2:.4f} | n={nrows}\")\n",
    "        out_path = out_pred_dir / f\"monthly_xco2_full_{tag}.npy\"\n",
    "        rec = df_month.to_records(index=False)\n",
    "        np.save(out_path, rec)\n",
    "\n",
    "        results.append({\n",
    "            \"month\": str(p), \n",
    "            \"year\": p.year,\n",
    "            \"mon\": p.month,\n",
    "            \"n\": nrows,\n",
    "            \"bias\": bias,\n",
    "            \"rmse\": rmse,\n",
    "            \"r2\": r2,\n",
    "        })\n",
    "        del arr, df_month, rec\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR]  {tag} ：{e}\")\n",
    "\n",
    "if results:\n",
    "    df_metrics = pd.DataFrame(results).sort_values([\"year\", \"mon\"]).reset_index(drop=True)\n",
    "    df_metrics.to_csv(metrics_path, index=False)\n",
    "    print(f\"✅ All completed：{metrics_path}\")\n",
    "else:\n",
    "    print(\"Failed\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_jupyter_env)",
   "language": "python",
   "name": "my_jupyter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
