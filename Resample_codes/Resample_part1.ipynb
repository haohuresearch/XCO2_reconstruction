{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60fe511a-70e2-4427-b552-c4c8ffae1bf8",
   "metadata": {},
   "source": [
    "# **Step 1: Create Fish Net**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc16d46-40cc-4106-b950-277c82c66265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "lat_res = 0.1\n",
    "lon_res = 0.1\n",
    "lats = np.arange(-89.95, 90.05, lat_res)\n",
    "lons = np.arange(-179.95, 180.05, lon_res)\n",
    "times = np.arange(np.datetime64(\"2019-01-01\"), np.datetime64(\"2025-07-01\"))\n",
    "\n",
    "ds_empty = xr.Dataset(coords={\n",
    "    \"lat\": lats,\n",
    "    \"lon\": lons,\n",
    "    \"time\": times\n",
    "})\n",
    "\n",
    "output_path = \"global_grid_0.1_2019_2025.nc\"\n",
    "ds_empty.to_netcdf(output_path)\n",
    "\n",
    "ds_empty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6656c0-e52d-4b78-aaa9-9af18a2ea88d",
   "metadata": {},
   "source": [
    "# **Step 2: Resample XCO2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09ebaf8-bc66-44f2-8a1e-a465b332aa11",
   "metadata": {},
   "source": [
    "### **OCO-2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815cbdab-feed-466c-99b3-aa5fea2e16eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "oco2_folder = \"../OCO2-HaoHu/\"\n",
    "grid_nc_path = \"global_grid_0.1_2019_2025.nc\"\n",
    "output_nc_path = \"global_grid_0.1_2019_2025_OCO-2.nc\"\n",
    "\n",
    "start_date = datetime.strptime(\"2019-01-01\", \"%Y-%m-%d\")\n",
    "end_date   = datetime.strptime(\"2025-07-01\", \"%Y-%m-%d\")\n",
    "\n",
    "ds_out = xr.open_dataset(grid_nc_path)\n",
    "lat = ds_out[\"lat\"].values\n",
    "lon = ds_out[\"lon\"].values\n",
    "time_all = ds_out[\"time\"].values\n",
    "time_len = len(ds_out[\"time\"])\n",
    "lat_len = len(ds_out[\"lat\"])\n",
    "lon_len = len(ds_out[\"lon\"])\n",
    "\n",
    "xco2_all = xr.DataArray(\n",
    "    np.full((time_len, lat_len, lon_len), np.nan, dtype=np.float32),\n",
    "    coords={\n",
    "        \"time\": ds_out[\"time\"],\n",
    "        \"lat\": ds_out[\"lat\"],\n",
    "        \"lon\": ds_out[\"lon\"]\n",
    "    },\n",
    "    dims=[\"time\", \"lat\", \"lon\"],\n",
    "    name=\"xco2_oco2\"\n",
    ")\n",
    "\n",
    "oco2_uncertainty_all = xr.DataArray(\n",
    "    np.full((time_len, lat_len, lon_len), np.nan, dtype=np.float32),\n",
    "    coords={\n",
    "        \"time\": ds_out[\"time\"],\n",
    "        \"lat\": ds_out[\"lat\"],\n",
    "        \"lon\": ds_out[\"lon\"]\n",
    "    },\n",
    "    dims=[\"time\", \"lat\", \"lon\"],\n",
    "    name=\"oco2_uncertainty\"\n",
    ")\n",
    "\n",
    "lat_res = lat[1] - lat[0]\n",
    "lon_res = lon[1] - lon[0]\n",
    "grid_shape = (len(lat), len(lon))\n",
    "half_box = int(0.05 / lat_res) \n",
    "\n",
    "def fast_idw(lat0s, lon0s, obs_lat, obs_lon, obs_val, radius):\n",
    "    results = np.full_like(lat0s, np.nan, dtype=np.float32)\n",
    "    for idx, (lat0, lon0) in enumerate(zip(lat0s, lon0s)):\n",
    "        d = np.sqrt((obs_lat - lat0)**2 + (obs_lon - lon0)**2)\n",
    "        mask = d <= radius\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "        w = 1 / (d[mask]**2 + 1e-12)\n",
    "        results[idx] = np.sum(w * obs_val[mask]) / np.sum(w)\n",
    "    return results\n",
    "\n",
    "files = sorted(glob(os.path.join(oco2_folder, \"*.nc4\")))\n",
    "files_by_day = defaultdict(list)\n",
    "\n",
    "for f in files:\n",
    "    try:\n",
    "        date_str = os.path.basename(f).split(\"_\")[2]  # e.g., '190101'\n",
    "        date_obj = datetime.strptime(date_str, \"%y%m%d\")\n",
    "\n",
    "        if not (start_date <= date_obj <= end_date):\n",
    "            continue\n",
    "\n",
    "        files_by_day[date_obj.strftime(\"%Y-%m-%d\")].append(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing date for file {f}: {e}\")\n",
    "        continue\n",
    "\n",
    "for date_str, filelist in tqdm(files_by_day.items(), desc=\"Processing days\"):\n",
    "    try:\n",
    "        date_index = np.where(np.datetime_as_string(time_all, unit='D') == date_str)[0][0]\n",
    "    except IndexError:\n",
    "        print(f\"Date {date_str} not in daily grid, skipping.\")\n",
    "        continue\n",
    "\n",
    "    lat_all, lon_all, xco2_all_day, unc_all_day = [], [], [], []  # === 新增 unc_all_day\n",
    "    for file in filelist:\n",
    "        try:\n",
    "            with xr.open_dataset(file) as ds:\n",
    "                mask = (ds[\"xco2_quality_flag\"].values == 0) & np.isfinite(ds[\"xco2\"].values)\n",
    "                lat_all.append(ds[\"latitude\"].values[mask])\n",
    "                lon_all.append(ds[\"longitude\"].values[mask])\n",
    "                xco2_all_day.append(ds[\"xco2\"].values[mask])\n",
    "                unc_all_day.append(ds[\"xco2_uncertainty\"].values[mask])  # === 新增\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if not lat_all:\n",
    "        continue\n",
    "\n",
    "    lats = np.concatenate(lat_all)\n",
    "    lons = np.concatenate(lon_all)\n",
    "    xco2_vals = np.concatenate(xco2_all_day)\n",
    "    unc_vals  = np.concatenate(unc_all_day) \n",
    "    \n",
    "    lat_idx = ((lats - lat[0]) / lat_res).astype(int)\n",
    "    lon_idx = ((lons - lon[0]) / lon_res).astype(int)\n",
    "    valid_mask = np.zeros(grid_shape, dtype=bool)\n",
    "\n",
    "    for i, j in zip(lat_idx, lon_idx):\n",
    "        if 0 <= i < grid_shape[0] and 0 <= j < grid_shape[1]:\n",
    "            i_min = max(0, i - half_box)\n",
    "            i_max = min(grid_shape[0], i + half_box + 1)\n",
    "            j_min = max(0, j - half_box)\n",
    "            j_max = min(grid_shape[1], j + half_box + 1)\n",
    "            valid_mask[i_min:i_max, j_min:j_max] = True\n",
    "\n",
    "    valid_i, valid_j = np.where(valid_mask)\n",
    "    if len(valid_i) == 0:\n",
    "        print(f\"No valid grid cells found for {date_str}\")\n",
    "        continue\n",
    "\n",
    "    lat0s = lat[valid_i]\n",
    "    lon0s = lon[valid_j]\n",
    "\n",
    "    interpolated_vals = fast_idw(lat0s, lon0s, lats, lons, xco2_vals, radius=0.2)\n",
    "    interpolated_unc  = fast_idw(lat0s, lon0s, lats, lons, unc_vals,  radius=0.2)  \n",
    "\n",
    "    out_grid = np.full(grid_shape, np.nan, dtype=np.float32)\n",
    "    out_grid[valid_i, valid_j] = interpolated_vals\n",
    "    xco2_all[date_index, :, :] = out_grid\n",
    "\n",
    "    out_unc = np.full(grid_shape, np.nan, dtype=np.float32)  \n",
    "    out_unc[valid_i, valid_j] = interpolated_unc           \n",
    "    oco2_uncertainty_all[date_index, :, :] = out_unc    \n",
    "    \n",
    "    print(f\"Filled {date_str} with {len(valid_i)} valid grid cells\")\n",
    "\n",
    "xco2_all.name = \"xco2_oco2\"\n",
    "ds_out[\"xco2_oco2\"] = xco2_all\n",
    "ds_out[\"oco2_uncertainty\"] = oco2_uncertainty_all  \n",
    "ds_out.to_netcdf(output_nc_path)\n",
    "output_nc_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5799be2d-e568-4f90-98ad-e8fede1b7b35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "oco2_folder = \"../OCO3-HaoHu/\"\n",
    "grid_nc_path = \"global_grid_0.1_2019_2025_OCO-2.nc\"\n",
    "output_nc_path = \"global_grid_0.1_2019_2025_OCO-2_3.nc\"\n",
    "\n",
    "start_date = datetime.strptime(\"2019-01-01\", \"%Y-%m-%d\")\n",
    "end_date   = datetime.strptime(\"2025-07-01\", \"%Y-%m-%d\")\n",
    "\n",
    "ds_out = xr.open_dataset(grid_nc_path)\n",
    "lat = ds_out[\"lat\"].values\n",
    "lon = ds_out[\"lon\"].values\n",
    "time_all = ds_out[\"time\"].values\n",
    "time_len = len(ds_out[\"time\"])\n",
    "lat_len = len(ds_out[\"lat\"])\n",
    "lon_len = len(ds_out[\"lon\"])\n",
    "\n",
    "xco2_all = xr.DataArray(\n",
    "    np.full((time_len, lat_len, lon_len), np.nan, dtype=np.float32),\n",
    "    coords={\n",
    "        \"time\": ds_out[\"time\"],\n",
    "        \"lat\": ds_out[\"lat\"],\n",
    "        \"lon\": ds_out[\"lon\"]\n",
    "    },\n",
    "    dims=[\"time\", \"lat\", \"lon\"],\n",
    "    name=\"xco2_oco3\"\n",
    ")\n",
    "\n",
    "oco2_uncertainty_all = xr.DataArray(\n",
    "    np.full((time_len, lat_len, lon_len), np.nan, dtype=np.float32),\n",
    "    coords={\n",
    "        \"time\": ds_out[\"time\"],\n",
    "        \"lat\": ds_out[\"lat\"],\n",
    "        \"lon\": ds_out[\"lon\"]\n",
    "    },\n",
    "    dims=[\"time\", \"lat\", \"lon\"],\n",
    "    name=\"oco3_uncertainty\"\n",
    ")\n",
    "\n",
    "lat_res = lat[1] - lat[0]\n",
    "lon_res = lon[1] - lon[0]\n",
    "grid_shape = (len(lat), len(lon))\n",
    "half_box = int(0.05 / lat_res) \n",
    "\n",
    "def fast_idw(lat0s, lon0s, obs_lat, obs_lon, obs_val, radius):\n",
    "    results = np.full_like(lat0s, np.nan, dtype=np.float32)\n",
    "    for idx, (lat0, lon0) in enumerate(zip(lat0s, lon0s)):\n",
    "        d = np.sqrt((obs_lat - lat0)**2 + (obs_lon - lon0)**2)\n",
    "        mask = d <= radius\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "        w = 1 / (d[mask]**2 + 1e-12)\n",
    "        results[idx] = np.sum(w * obs_val[mask]) / np.sum(w)\n",
    "    return results\n",
    "\n",
    "files = sorted(glob(os.path.join(oco2_folder, \"*.nc4\")))\n",
    "files_by_day = defaultdict(list)\n",
    "\n",
    "for f in files:\n",
    "    try:\n",
    "        date_str = os.path.basename(f).split(\"_\")[2]  # e.g., '190101'\n",
    "        date_obj = datetime.strptime(date_str, \"%y%m%d\")\n",
    "\n",
    "        if not (start_date <= date_obj <= end_date):\n",
    "            continue\n",
    "\n",
    "        files_by_day[date_obj.strftime(\"%Y-%m-%d\")].append(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing date for file {f}: {e}\")\n",
    "        continue\n",
    "\n",
    "for date_str, filelist in tqdm(files_by_day.items(), desc=\"Processing days\"):\n",
    "    try:\n",
    "        date_index = np.where(np.datetime_as_string(time_all, unit='D') == date_str)[0][0]\n",
    "    except IndexError:\n",
    "        print(f\"Date {date_str} not in daily grid, skipping.\")\n",
    "        continue\n",
    "\n",
    "    lat_all, lon_all, xco2_all_day, unc_all_day = [], [], [], [] \n",
    "    for file in filelist:\n",
    "        try:\n",
    "            with xr.open_dataset(file) as ds:\n",
    "                mask = (ds[\"xco2_quality_flag\"].values == 0) & np.isfinite(ds[\"xco2\"].values)\n",
    "                lat_all.append(ds[\"latitude\"].values[mask])\n",
    "                lon_all.append(ds[\"longitude\"].values[mask])\n",
    "                xco2_all_day.append(ds[\"xco2\"].values[mask])\n",
    "                unc_all_day.append(ds[\"xco2_uncertainty\"].values[mask]) \n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if not lat_all:\n",
    "        continue\n",
    "\n",
    "    lats = np.concatenate(lat_all)\n",
    "    lons = np.concatenate(lon_all)\n",
    "    xco2_vals = np.concatenate(xco2_all_day)\n",
    "    unc_vals  = np.concatenate(unc_all_day)  \n",
    "\n",
    "    lat_idx = ((lats - lat[0]) / lat_res).astype(int)\n",
    "    lon_idx = ((lons - lon[0]) / lon_res).astype(int)\n",
    "    valid_mask = np.zeros(grid_shape, dtype=bool)\n",
    "\n",
    "    for i, j in zip(lat_idx, lon_idx):\n",
    "        if 0 <= i < grid_shape[0] and 0 <= j < grid_shape[1]:\n",
    "            i_min = max(0, i - half_box)\n",
    "            i_max = min(grid_shape[0], i + half_box + 1)\n",
    "            j_min = max(0, j - half_box)\n",
    "            j_max = min(grid_shape[1], j + half_box + 1)\n",
    "            valid_mask[i_min:i_max, j_min:j_max] = True\n",
    "\n",
    "    valid_i, valid_j = np.where(valid_mask)\n",
    "    if len(valid_i) == 0:\n",
    "        print(f\"No valid grid cells found for {date_str}\")\n",
    "        continue\n",
    "\n",
    "    lat0s = lat[valid_i]\n",
    "    lon0s = lon[valid_j]\n",
    "\n",
    "    interpolated_vals = fast_idw(lat0s, lon0s, lats, lons, xco2_vals, radius=0.2)\n",
    "    interpolated_unc  = fast_idw(lat0s, lon0s, lats, lons, unc_vals,  radius=0.2)  \n",
    "    out_grid = np.full(grid_shape, np.nan, dtype=np.float32)\n",
    "    out_grid[valid_i, valid_j] = interpolated_vals\n",
    "    xco2_all[date_index, :, :] = out_grid\n",
    "\n",
    "    out_unc = np.full(grid_shape, np.nan, dtype=np.float32) \n",
    "    out_unc[valid_i, valid_j] = interpolated_unc            \n",
    "    oco2_uncertainty_all[date_index, :, :] = out_unc      \n",
    "    \n",
    "    print(f\"Filled {date_str} with {len(valid_i)} valid grid cells\")\n",
    "\n",
    "xco2_all.name = \"xco2_oco3\"\n",
    "ds_out[\"xco2_oco3\"] = xco2_all\n",
    "ds_out[\"oco3_uncertainty\"] = oco2_uncertainty_all  \n",
    "ds_out.to_netcdf(output_nc_path)\n",
    "output_nc_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399de1df-7486-4cac-b58a-706ef1f3fb16",
   "metadata": {},
   "source": [
    "## Merge OCO2 and OCO3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30949cfa-442b-4aec-99d6-5b0d6a024c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "ds = xr.open_dataset(\"global_grid_0.1_2019_2025_OCO-2_3.nc\")\n",
    "\n",
    "xco2_oco2 = ds.get(\"xco2_oco2\")\n",
    "xco2_oco3 = ds.get(\"xco2_oco3\")\n",
    "uncertainty_oco2 = ds.get(\"oco2_uncertainty\")\n",
    "uncertainty_oco3 = ds.get(\"oco3_uncertainty\")\n",
    "\n",
    "combined_xco2 = xr.full_like(xco2_oco2, fill_value=np.nan)\n",
    "\n",
    "for t in tqdm(range(xco2_oco2.sizes[\"time\"]), desc=\"Combining xco2\"):\n",
    "\n",
    "    x2 = xco2_oco2.isel(time=t)\n",
    "    x3 = xco2_oco3.isel(time=t)\n",
    "    u2 = uncertainty_oco2.isel(time=t)\n",
    "    u3 = uncertainty_oco3.isel(time=t)\n",
    "\n",
    "    both_valid = np.isfinite(x2) & np.isfinite(x3)\n",
    "    only_oco2 = np.isfinite(x2) & ~np.isfinite(x3)\n",
    "    only_oco3 = ~np.isfinite(x2) & np.isfinite(x3)\n",
    "\n",
    "    result = xr.full_like(x2, fill_value=np.nan)\n",
    "\n",
    "    result = result.where(~only_oco2, x2)\n",
    "\n",
    "    result = result.where(~only_oco3, x3)\n",
    "\n",
    "    w2 = 1 / (u2 + 1e-6)**2\n",
    "    w3 = 1 / (u3 + 1e-6)**2\n",
    "    weighted = (x2 * w2 + x3 * w3) / (w2 + w3)\n",
    "    result = result.where(~both_valid, weighted)\n",
    "\n",
    "    combined_xco2[t] = result\n",
    "\n",
    "out_ds = xr.Dataset({\"xco2\": combined_xco2})\n",
    "out_ds.to_netcdf(\"global_grid_0.1_2019_2025_xco2.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65426d07-57ac-4f96-8259-1e6766459a9e",
   "metadata": {},
   "source": [
    "## print the number of non-NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca98b85-98df-4eeb-a81b-f5a3c9413419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "ds = xr.open_dataset(\"global_grid_0.1_2019_2025_OCO-2_3.nc\")\n",
    "\n",
    "xco2_oco3 = ds[\"xco2_oco3\"]\n",
    "xco2_oco2 = ds[\"xco2_oco2\"]\n",
    "\n",
    "print(\"OCO-3   non-NaN\", xco2_oco3.count().item())\n",
    "print(\"OCO-2   non-NaN:\", xco2_oco2.count().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d18135-54a9-49c2-909b-da08ffba03dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "ds = xr.open_dataset(\"global_grid_0.1_2019_2025_OCO-2_3.nc\")\n",
    "\n",
    "xco2_oco2 = ds[\"xco2_oco2\"]\n",
    "xco2_oco3 = ds[\"xco2_oco3\"]\n",
    "\n",
    "mask = (~np.isnan(xco2_oco2)) & (~np.isnan(xco2_oco3))\n",
    "\n",
    "diff = xco2_oco2 - xco2_oco3  # shape: [time, lat, lon]\n",
    "\n",
    "valid_diff = diff.where(mask)\n",
    "\n",
    "mean_diff = valid_diff.mean().item()\n",
    "std_diff = valid_diff.std().item()\n",
    "\n",
    "mask_anytime = mask.any(dim=\"time\")\n",
    "count = mask_anytime.sum().item()\n",
    "\n",
    "print(f\"同时有xco2_oco2和xco2_oco3的网格点数量: {count}\")\n",
    "print(f\"OCO-2 和 OCO-3 的XCO2平均差值: {mean_diff:.4f}\")\n",
    "print(f\"OCO-2 和 OCO-3 的XCO2标准差: {std_diff:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ad7aa3-6054-45d3-8922-7aef5ac2e9f4",
   "metadata": {},
   "source": [
    "# Step 3: Resample NO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd871ff-c9b1-4457-a899-754bd8cdfdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_folder = \"/data3/interns/NRT_CO2_Emission_Map_Project/HaoHu_work/TROPOMI-NO2_HaoHu/\"\n",
    "target_grid_nc = \"global_grid_0.1_2019_2025.nc\"\n",
    "output_nc = \"global_grid_0.1_2019_2025_NO2.nc\"\n",
    "\n",
    "target_grid = xr.open_dataset(target_grid_nc)\n",
    "target_lat = target_grid['lat']\n",
    "target_lon = target_grid['lon']\n",
    "target_time = target_grid['time'].values\n",
    "\n",
    "file_list = sorted(glob(os.path.join(input_folder, \"EOSDISNO2_20????_15d.nc\")))\n",
    "\n",
    "file_dict = {}\n",
    "for file in file_list:\n",
    "    filename = os.path.basename(file)\n",
    "    year_month = filename.split('_')[1][:6]  # \"202001\"\n",
    "    file_dict[year_month] = file\n",
    "\n",
    "NO2_all = xr.DataArray(\n",
    "    np.full((len(target_time), len(target_lat), len(target_lon)), np.nan, dtype=np.float32),\n",
    "    coords={\"time\": target_time, \"lat\": target_lat, \"lon\": target_lon},\n",
    "    dims=[\"time\", \"lat\", \"lon\"],\n",
    "    name=\"NO2\"\n",
    ")\n",
    "\n",
    "for i, time_point in enumerate(tqdm(target_time, desc=\"Interpolating NO2\")):\n",
    "    yyyymm = pd.to_datetime(str(time_point)).strftime(\"%Y%m\")\n",
    "    if yyyymm not in file_dict:\n",
    "        continue\n",
    "\n",
    "    ds = xr.open_dataset(file_dict[yyyymm])\n",
    "    if \"nday\" in ds.dims:\n",
    "        ds = ds.rename({\"nday\": \"time\"})\n",
    "\n",
    "    start_time = np.datetime64(f\"{yyyymm[:4]}-{yyyymm[4:]}-01\")\n",
    "    n_days = ds.sizes['time']\n",
    "    ds['time'] = start_time + np.arange(n_days)\n",
    "\n",
    "    if time_point not in ds['time']:\n",
    "        continue\n",
    "\n",
    "    no2_day = ds['NO2'].sel(time=time_point)\n",
    "    if \"latitude\" in no2_day.dims and \"longitude\" in no2_day.dims:\n",
    "        no2_day = no2_day.rename({\"latitude\": \"lat\", \"longitude\": \"lon\"})\n",
    "\n",
    "    interp_day = no2_day.interp(lat=target_lat, lon=target_lon, method=\"linear\")\n",
    "\n",
    "    interp_day = interp_day.where(interp_day >= 0, 0)\n",
    "    \n",
    "    NO2_all[i, :, :] = interp_day.values\n",
    "\n",
    "out_ds = xr.Dataset({\"NO2\": NO2_all})\n",
    "out_ds.to_netcdf(output_nc)\n",
    "\n",
    "output_nc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879a9b32-923f-4e1a-8f2f-db5cf244aed0",
   "metadata": {},
   "source": [
    "# Step 4: Weekend or Weekeday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0c13f4-b9fc-4fcf-9b44-385d1b2beca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_nc = \"global_grid_0.1_2019_2025.nc\"\n",
    "output_nc = \"global_grid_0.1_2019_2025_weekday_weekend.nc\"\n",
    "\n",
    "ds = xr.open_dataset(input_nc)\n",
    "time = ds['time']\n",
    "lat = ds['lat']\n",
    "lon = ds['lon']\n",
    "\n",
    "is_weekend_1d = xr.DataArray(\n",
    "    [1 if pd.Timestamp(t).weekday() >= 5 else 0 for t in tqdm(time.values, desc=\"whether weekend\")],\n",
    "    dims='time',\n",
    "    coords={'time': time},\n",
    "    name='is_weekend_flag',\n",
    "    attrs={\n",
    "        'description': '1 = weekend (Saturday or Sunday), 0 = weekday',\n",
    "        'long_name': 'Weekend Indicator (1D)',\n",
    "        'units': '1'\n",
    "    }\n",
    ")\n",
    "\n",
    "is_weekend_3d, _, _ = xr.broadcast(is_weekend_1d, lat, lon)\n",
    "is_weekend_3d.name = 'is_weekend'\n",
    "is_weekend_3d.attrs = {\n",
    "    'description': '1 = weekend (Saturday or Sunday), 0 = weekday',\n",
    "    'long_name': 'Weekend Indicator (broadcasted to 3D)',\n",
    "    'units': '1'\n",
    "}\n",
    "\n",
    "ds['is_weekend'] = is_weekend_3d\n",
    "ds.to_netcdf(output_nc)\n",
    "print(f\"✅ Save: {output_nc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
